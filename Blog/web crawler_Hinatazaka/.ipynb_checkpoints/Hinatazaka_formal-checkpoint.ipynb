{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3b82705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\harry\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\harry\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: pip in c:\\users\\harry\\anaconda3\\lib\\site-packages (22.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cb5576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,bs4,os,json,time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dabd5f",
   "metadata": {},
   "source": [
    "## download html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0154bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hinatazaka46.com/s/official/diary/member/list?ima=0000'\n",
    "# html = requests.get(url)\n",
    "# print('網頁下載中')\n",
    "# html.raise_for_status()\n",
    "# print('網頁下載成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455ce72",
   "metadata": {},
   "source": [
    "## create Nogizaka46 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58682855",
   "metadata": {},
   "outputs": [],
   "source": [
    "deskDir = 'Hinatazaka46'\n",
    "if os.path.exists(deskDir) == False:\n",
    "    os.mkdir(deskDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b7705",
   "metadata": {},
   "source": [
    "## find blog links on current page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "403db7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(current_objSoup):\n",
    "    global current_page,download_complete\n",
    "    nextPages = current_objSoup.find_all('li', class_ = 'c-pager__item--count')\n",
    "    \n",
    "    for i in range(len(nextPages)):\n",
    "        nextpage = nextPages[i].find('a')\n",
    "        time.sleep(0.5)\n",
    "        if  nextpage is not None and nextpage.text == str(current_page+1):\n",
    "            current_page += 1\n",
    "            link = nextpage['href']\n",
    "            return url[:28] + link\n",
    "        else:\n",
    "            continue\n",
    "    #it is end\n",
    "    download_complete = True\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b13e90ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_pictures(blog_url):\n",
    "    global current_new,download_complete,current_page\n",
    "    #url = https://www.nogizaka46.com/s/n46/diary/detail/100577?ima=2936&cd=MEMBER\n",
    "    html = requests.get(blog_url)\n",
    "    time.sleep(1)\n",
    "    objSoup = bs4.BeautifulSoup(html.text,'lxml')\n",
    "    \n",
    "    blog_tags = objSoup.find_all('div', class_ = \"c-blog-article__text\")\n",
    "#     print(len(imgTag))\n",
    "\n",
    "    author_members_raw = objSoup.find_all('div',class_ = 'c-blog-article__name')\n",
    "    author_members = []\n",
    "    for i in author_members_raw:\n",
    "        author_members.append(i.text.strip())\n",
    "        \n",
    "    blog_title_raw = objSoup.find_all('div', class_ = \"c-blog-article__title\")\n",
    "    blog_titles = []\n",
    "    for i in blog_title_raw:\n",
    "        bt = i.text\n",
    "        if bt is None:\n",
    "            bt = ''\n",
    "        blog_titles.append(bt.strip())\n",
    "        \n",
    "    dateTime_raw = objSoup.find_all('div', class_ = \"c-blog-article__date\")\n",
    "    dates = []\n",
    "    for i in dateTime_raw:\n",
    "        dateTime = i.text.strip() #get '2022.8.15 18:30'\n",
    "        dateTime = dateTime.split('.')\n",
    "        year,month,day = dateTime[0],dateTime[1].zfill(2),dateTime[2][:-6].zfill(2)\n",
    "\n",
    "        dates.append(year+month+day)\n",
    "    \n",
    "    #run for 20 times in 1 page\n",
    "    for k in range(len(blog_tags)):\n",
    "        imgTag = blog_tags[k].find_all('img')\n",
    "        author_member = author_members[k]\n",
    "        blog_title = blog_titles[k]\n",
    "        date = dates[k]\n",
    "        \n",
    "    \n",
    "        data = [dateTime_raw[k].text.strip(),author_member,blog_title]\n",
    "        if data == current_new: #we can stop here\n",
    "            download_complete = True\n",
    "        else:\n",
    "            #blog_title may not a good directory name\n",
    "            blog_title = blog_title[:150]\n",
    "            special_char = r'/\\:*?\"<>|'\n",
    "            for i in special_char:\n",
    "                blog_title = blog_title.replace(i,'')\n",
    "\n",
    "            dir_path_member = './Hinatazaka46/' + author_member\n",
    "            dir_path = './Hinatazaka46/' + author_member + '/' + date + ' ' + blog_title\n",
    "\n",
    "            #the space at the end of blog_title should be remove\n",
    "            dir_path = dir_path.rstrip(' .')\n",
    "\n",
    "            if os.path.exists(dir_path_member) == False:\n",
    "                os.mkdir(dir_path_member)\n",
    "            if os.path.exists(dir_path) == False:\n",
    "                os.mkdir(dir_path)\n",
    "                if len(imgTag) > 0 :\n",
    "                    for i in range(len(imgTag)):\n",
    "                        imgUrl = imgTag[i].get('src')\n",
    "\n",
    "                        try:\n",
    "                            is_effective_url = (not imgUrl is None) and ((imgUrl[-4:] in ['.jpg','.tif','.png']) or (imgUrl[-5:] in ['.jpeg']))\n",
    "                            if is_effective_url: #we may catch something we don't want(no 'src') \n",
    "                                \n",
    "                                picture = requests.get(imgUrl)\n",
    "                                time.sleep(0.5)\n",
    "                                picture.raise_for_status()\n",
    "                #                 print('%s 圖片下載成功' %finUrl)\n",
    "\n",
    "                                pictFile = open(os.path.join(dir_path,str(i).zfill(2) + (imgUrl[-4:] if imgUrl[-4:] in ['.jpg','.tif','.png'] else imgUrl[-5:])),'wb')\n",
    "                                for diskStorage in picture.iter_content(1024*1024):\n",
    "                                    pictFile.write(diskStorage)\n",
    "                                pictFile.close()\n",
    "                        except requests.exceptions.HTTPError:\n",
    "                            print(current_page,k+1,'找不到網頁! 照片可能被鎖了',author_member,blog_title)\n",
    "\n",
    "                        except:\n",
    "                            print(current_page,k+1,'新的未知錯誤出現! 可能是異常個案',author_member,blog_title)\n",
    "            else:\n",
    "                print('看到重複的部落格了')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e76026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_current_new(current_new_url):\n",
    "    html = requests.get(current_new_url)\n",
    "    time.sleep(1)\n",
    "    objSoup = bs4.BeautifulSoup(html.text,'lxml')\n",
    "    \n",
    "    blog_tags = objSoup.find_all('div', class_ = \"c-blog-article__text\")\n",
    "    \n",
    "    author_members_raw = objSoup.find_all('div',class_ = 'c-blog-article__name')\n",
    "    author_member = author_members_raw[0].text.strip()\n",
    "    \n",
    "    blog_title_raw = objSoup.find_all('div', class_ = \"c-blog-article__title\")\n",
    "    bt = blog_title_raw[0].text\n",
    "    if bt is None:\n",
    "        blog_title = ''\n",
    "    else:\n",
    "        blog_title = bt.strip()\n",
    "    \n",
    "    dateTime_raw = objSoup.find_all('div', class_ = \"c-blog-article__date\")\n",
    "    dateTime = dateTime_raw[0].text.strip() #get '2022.8.15 18:30'\n",
    "    \n",
    "    current_new_data = [dateTime,author_member,blog_title]\n",
    "    \n",
    "    fn = 'current_new.json'\n",
    "    with open(fn,'w',encoding='utf-8') as obj:\n",
    "        json.dump(current_new_data,obj,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12a734a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blogs(current_objSoup):\n",
    "    global current_new,current_page,download_complete\n",
    "    blogData = current_objSoup.find_all('a',class_='bl--card js-pos a--op hv--thumb')\n",
    "    \n",
    "    for i in range(len(blogData)):\n",
    "        link = blogData[i]['href']\n",
    "        blog_url = url[:26] + link\n",
    "        \n",
    "        if not download_complete:\n",
    "            download_pictures(blog_url,i)\n",
    "        \n",
    "        if current_page == 1 and i == 0: #always update first\n",
    "            update_current_new(blog_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79599186",
   "metadata": {},
   "source": [
    "## set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed0af820",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page = 1\n",
    "current_url = url\n",
    "\n",
    "download_complete = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7261c1a",
   "metadata": {},
   "source": [
    "## get current new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "829fa4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'current_new.json'\n",
    "\n",
    "try:\n",
    "    with open(fn,'r',encoding='utf-8') as fnobj:\n",
    "        current_new = json.load(fnobj)\n",
    "\n",
    "except Exception: #the 1st run of this code\n",
    "    current_new = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2f572",
   "metadata": {},
   "source": [
    "## main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c3544d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n",
      "看到重複的部落格了\n"
     ]
    }
   ],
   "source": [
    "while not download_complete:\n",
    "    print(current_page)\n",
    "    if current_page == 1:\n",
    "        update_current_new(current_url)\n",
    "        \n",
    "    html = requests.get(current_url)\n",
    "    current_objSoup = bs4.BeautifulSoup(html.text,'lxml')\n",
    "    try:\n",
    "        download_pictures(current_url)\n",
    "    except FileExistsError as e:\n",
    "        print('檔案已存在! 載到重複的部落格')\n",
    "        break\n",
    "    except FileNotFoundError as e:\n",
    "        print('檔案找不到! 可能部落格名稱太長或路徑有錯')\n",
    "        break\n",
    "    except NotADirectoryError as e:\n",
    "        print('目錄名稱無效! 可能資料夾命名不合法')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print( e,'可能為異常個案')\n",
    "    current_url = get_next_page(current_objSoup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
